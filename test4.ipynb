{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9aa6a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bootcamp\\AppData\\Local\\Temp\\ipykernel_24904\\1393138689.py:71: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  model = ChatOllama(model=\"gemma3:4b\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 108\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 108\u001b[0m         response \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke({\n\u001b[0;32m    109\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk,\n\u001b[0;32m    110\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_instructions\u001b[39m\u001b[38;5;124m\"\u001b[39m: parser\u001b[38;5;241m.\u001b[39mget_format_instructions()\n\u001b[0;32m    111\u001b[0m         })\n\u001b[0;32m    112\u001b[0m         cleaned_data \u001b[38;5;241m=\u001b[39m clean_response_data(response)\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m cleaned_data:\n",
      "File \u001b[1;32mc:\\Users\\bootcamp\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3047\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3045\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3046\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3047\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config)\n\u001b[0;32m   3048\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3049\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\bootcamp\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:372\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    368\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    369\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 372\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    373\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    374\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    375\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    376\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    377\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    378\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    379\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    380\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    381\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    382\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\bootcamp\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:957\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    955\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    956\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bootcamp\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:776\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[0;32m    774\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    775\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 776\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    777\u001b[0m                 m,\n\u001b[0;32m    778\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    779\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    780\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    781\u001b[0m             )\n\u001b[0;32m    782\u001b[0m         )\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\bootcamp\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1022\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1022\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m   1023\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1024\u001b[0m     )\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1026\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bootcamp\\anaconda3\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py:291\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    269\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m    274\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to Ollama's generate endpoint.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;124;03m            ])\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_stream_with_aggregation(\n\u001b[0;32m    292\u001b[0m         messages,\n\u001b[0;32m    293\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    294\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    295\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[0;32m    298\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[0;32m    299\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext),\n\u001b[0;32m    300\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mgeneration_info,\n\u001b[0;32m    301\u001b[0m     )\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[chat_generation])\n",
      "File \u001b[1;32mc:\\Users\\bootcamp\\anaconda3\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py:222\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[1;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    215\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    220\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[0;32m    221\u001b[0m     final_chunk: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[0;32m    224\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _chat_stream_response_to_chat_generation_chunk(stream_resp)\n",
      "File \u001b[1;32mc:\\Users\\bootcamp\\anaconda3\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py:194\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[1;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_chat_stream\u001b[39m(\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    186\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m    187\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    189\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    190\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_messages_to_ollama_messages(messages),\n\u001b[0;32m    193\u001b[0m     }\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[0;32m    195\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload, stop\u001b[38;5;241m=\u001b[39mstop, api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    196\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\bootcamp\\anaconda3\\Lib\\site-packages\\requests\\models.py:869\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[1;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;124;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;124;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \n\u001b[0;32m    864\u001b[0m \u001b[38;5;124;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    867\u001b[0m pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(\n\u001b[0;32m    870\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, decode_unicode\u001b[38;5;241m=\u001b[39mdecode_unicode\n\u001b[0;32m    871\u001b[0m ):\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pending \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m pending \u001b[38;5;241m+\u001b[39m chunk\n",
      "File \u001b[1;32mc:\\Users\\bootcamp\\anaconda3\\Lib\\site-packages\\requests\\utils.py:572\u001b[0m, in \u001b[0;36mstream_decode_response_unicode\u001b[1;34m(iterator, r)\u001b[0m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    571\u001b[0m decoder \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mgetincrementaldecoder(r\u001b[38;5;241m.\u001b[39mencoding)(errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 572\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[0;32m    573\u001b[0m     rv \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(chunk)\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rv:\n",
      "File \u001b[1;32mc:\\Users\\bootcamp\\anaconda3\\Lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\bootcamp\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:624\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content):\n\u001b[0;32m    625\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m line\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\bootcamp\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:828\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 828\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_chunk_length()\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    830\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bootcamp\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:758\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 758\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline()\n\u001b[0;32m    759\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\bootcamp\\anaconda3\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Re-run the code after kernel reset to ensure continuity\n",
    "\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Define the model schema\n",
    "class PVArticleData(BaseModel):\n",
    "    title: str = Field(\"N/A\")\n",
    "    last_name: str = Field(\"N/A\")\n",
    "    year: str = Field(\"N/A\")\n",
    "    doi: str = Field(\"N/A\")\n",
    "    research_focus: str = Field(\"N/A\")\n",
    "    key_findings: str = Field(\"N/A\")\n",
    "    device_type: str = Field(\"N/A\")\n",
    "    absorber_material: str = Field(\"N/A\")\n",
    "    absorber_material_term_used: str = Field(\"N/A\")\n",
    "    absorber_dopant_material: str = Field(\"N/A\")\n",
    "    absorber_dopant_material_term_used: str = Field(\"N/A\")\n",
    "    absorber_dopant_polarity: str = Field(\"N/A\")\n",
    "    absorber_dopant_polarity_term_used: str = Field(\"N/A\")\n",
    "    front_surface_morphology: str = Field(\"N/A\")\n",
    "    front_surface_morphology_term_used: str = Field(\"N/A\")\n",
    "    rear_surface_morphology: str = Field(\"N/A\")\n",
    "    rear_surface_morphology_term_used: str = Field(\"N/A\")\n",
    "    front_surface_passivation_material: str = Field(\"N/A\")\n",
    "    front_surface_passivation_material_term_used: str = Field(\"N/A\")\n",
    "    rear_surface_passivation_material: str = Field(\"N/A\")\n",
    "    rear_surface_passivation_material_term_used: str = Field(\"N/A\")\n",
    "    negative_metallization_material: str = Field(\"N/A\")\n",
    "    negative_metallization_material_term_used: str = Field(\"N/A\")\n",
    "    positive_metallization_material: str = Field(\"N/A\")\n",
    "    positive_metallization_material_term_used: str = Field(\"N/A\")\n",
    "    efficiency_percent: str = Field(\"N/A\")\n",
    "    cell_area_cm2: str = Field(\"N/A\")\n",
    "    short_circuit_current_a: str = Field(\"N/A\")\n",
    "    short_circuit_current_density_ma_cm2: str = Field(\"N/A\")\n",
    "    open_circuit_voltage_v: str = Field(\"N/A\")\n",
    "    fill_factor_percent: str = Field(\"N/A\")\n",
    "\n",
    "    @field_validator(\"*\", mode=\"before\")\n",
    "    def convert_to_string(cls, v):\n",
    "        return \"N/A\" if v is None else str(v)\n",
    "\n",
    "def clean_response_data(response_data):\n",
    "    if isinstance(response_data, list):\n",
    "        response_data = response_data[0] if response_data else {}\n",
    "    if isinstance(response_data, PVArticleData):\n",
    "        return response_data.model_dump()\n",
    "    if isinstance(response_data, str):\n",
    "        cleaned = response_data.strip().removeprefix(\"```json\").removeprefix(\"```\").removesuffix(\"```\").strip()\n",
    "        try:\n",
    "            parsed = json.loads(cleaned)\n",
    "            return parsed[0] if isinstance(parsed, list) else parsed\n",
    "        except json.JSONDecodeError:\n",
    "            return {}\n",
    "    if isinstance(response_data, dict):\n",
    "        return response_data\n",
    "    return {}\n",
    "\n",
    "def count_filled_fields(data: dict) -> int:\n",
    "    return sum(1 for v in data.values() if v != \"N/A\")\n",
    "\n",
    "# Setup model and prompt\n",
    "model = ChatOllama(model=\"gemma3:4b\")\n",
    "parser = PydanticOutputParser(pydantic_object=PVArticleData)\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are extracting structured data from academic articles on photovoltaic cells.\n",
    "Focus only on the most efficient cell mentioned in each article.\n",
    "\n",
    "IMPORTANT:\n",
    "- Return ALL fields from the schema.\n",
    "- If any field is not available, set its value to \"N/A\".\n",
    "- Return ONLY a single JSON object.\n",
    "- Do not use markdown or code formatting.\n",
    "\n",
    "Article:\n",
    "{text}\n",
    "\n",
    "Format like:\n",
    "{format_instructions}\n",
    "\"\"\")\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# File loading and processing\n",
    "pdf_folder = Path(\"PV1-Rhea\")\n",
    "pdf_files = list(pdf_folder.rglob(\"*.pdf\"))\n",
    "results = []\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    try:\n",
    "        loader = PDFMinerLoader(str(pdf_file))\n",
    "        docs = loader.load()\n",
    "        full_text = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "        chunks = splitter.split_text(full_text)\n",
    "\n",
    "        best_data = None\n",
    "        highest_filled_count = 0\n",
    "        for chunk in chunks:\n",
    "            try:\n",
    "                response = chain.invoke({\n",
    "                    \"text\": chunk,\n",
    "                    \"format_instructions\": parser.get_format_instructions()\n",
    "                })\n",
    "                cleaned_data = clean_response_data(response)\n",
    "                if cleaned_data:\n",
    "                    filled_count = count_filled_fields(cleaned_data)\n",
    "                    if filled_count > highest_filled_count:\n",
    "                        highest_filled_count = filled_count\n",
    "                        best_data = cleaned_data\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if best_data:\n",
    "            article_data = PVArticleData(**best_data)\n",
    "            results.append(article_data.model_dump())\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# Save the results\n",
    "if results:\n",
    "    df = pd.DataFrame(results)\n",
    "    column_map = {\n",
    "        \"title\": \"Title\", \"last_name\": \"Last Name\", \"year\": \"Year\", \"doi\": \"Digital Object Identifier (DOI)\",\n",
    "        \"research_focus\": \"Research Focus\", \"key_findings\": \"Key Findings\", \"device_type\": \"Device Type\",\n",
    "        \"absorber_material\": \"Absorber Material\", \"absorber_material_term_used\": \"Absorber Material Term Used\",\n",
    "        \"absorber_dopant_material\": \"Absorber Dopant Material\", \"absorber_dopant_material_term_used\": \"Absorber Dopant Material Term Used\",\n",
    "        \"absorber_dopant_polarity\": \"Absorber Dopant Polarity\", \"absorber_dopant_polarity_term_used\": \"Absorber Dopant Polarity Term Used\",\n",
    "        \"front_surface_morphology\": \"Front Surface Morphology\", \"front_surface_morphology_term_used\": \"Front Surface Morphology Term Used\",\n",
    "        \"rear_surface_morphology\": \"Rear Surface Morphology\", \"rear_surface_morphology_term_used\": \"Rear Surface Morphology Term Used\",\n",
    "        \"front_surface_passivation_material\": \"Front Surface Passivation Material\", \"front_surface_passivation_material_term_used\": \"Front Surface Passivation Material Term Used\",\n",
    "        \"rear_surface_passivation_material\": \"Rear Surface Passivation Material\", \"rear_surface_passivation_material_term_used\": \"Rear Surface Passivation Material Term Used\",\n",
    "        \"negative_metallization_material\": \"Negative Metallization Material\", \"negative_metallization_material_term_used\": \"Negative Metallization Material Term Used\",\n",
    "        \"positive_metallization_material\": \"Positive Metallization Material\", \"positive_metallization_material_term_used\": \"Positive Metallization Material Term Used\",\n",
    "        \"efficiency_percent\": \"Efficiency (%)\", \"cell_area_cm2\": \"Cell Area (cm2)\",\n",
    "        \"short_circuit_current_a\": \"Short-Circuit Current (A)\", \"short_circuit_current_density_ma_cm2\": \"Short-Circuit Current Density (mA/cm2)\",\n",
    "        \"open_circuit_voltage_v\": \"Open-Circuit Voltage (V)\", \"fill_factor_percent\": \"Fill Factor (%)\"\n",
    "    }\n",
    "    df = df.rename(columns=column_map)\n",
    "    df = df[list(column_map.values())]\n",
    "    df.to_csv(\"pv_extraction_results_ollama.csv\", index=False)\n",
    "    print(\"✅ Saved extracted data.\")\n",
    "else:\n",
    "    print(\"⚠️ No data extracted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2665b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812dcd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
