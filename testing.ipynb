{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff7a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "PDF_FOLDER = \"pdfs\"\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "MAX_CHUNKS_PER_PDF = 5\n",
    "MAX_WORKERS = 8\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.2:1b\")  # Shared LLM instance\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "executor = ThreadPoolExecutor(max_workers=MAX_WORKERS)\n",
    "\n",
    "def load_and_split_pdf(pdf_path):\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    return chunks[:MAX_CHUNKS_PER_PDF] if MAX_CHUNKS_PER_PDF else chunks\n",
    "\n",
    "def process_chunk_sync(content, filename, i):\n",
    "    try:\n",
    "        llm.invoke(content)\n",
    "        print(f\" {filename} | Chunk {i+1}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error on chunk {i+1} of {filename}: {e}\")\n",
    "\n",
    "async def process_pdf(filename):\n",
    "    if not filename.endswith(\".pdf\"):\n",
    "        return\n",
    "\n",
    "    pdf_path = os.path.join(PDF_FOLDER, filename)\n",
    "    try:\n",
    "        chunks = await asyncio.get_event_loop().run_in_executor(executor, load_and_split_pdf, pdf_path)\n",
    "\n",
    "        tasks = [\n",
    "            asyncio.get_event_loop().run_in_executor(\n",
    "                executor, process_chunk_sync, chunk.page_content, filename, i\n",
    "            )\n",
    "            for i, chunk in enumerate(chunks)\n",
    "        ]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to process {filename}: {e}\")\n",
    "\n",
    "async def main():\n",
    "    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.endswith(\".pdf\")]\n",
    "    tasks = [process_pdf(f) for f in pdf_files]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import nest_asyncio                 \n",
    "    nest_asyncio.apply()  # Allows nested use of asyncio.run()\n",
    "    asyncio.get_event_loop().run_until_complete(main())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ef0826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Configuration\n",
    "PDF_FOLDER = \"pdfs\"\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "MAX_CHUNKS_PER_PDF = 5\n",
    "MAX_WORKERS = os.cpu_count() or 8\n",
    "\n",
    "# Shared objects\n",
    "llm = OllamaLLM(model=\"llama3.2:1b\")\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "executor = ThreadPoolExecutor(max_workers=MAX_WORKERS)\n",
    "\n",
    "# Purely synchronous processing for one PDF file\n",
    "def process_pdf_sync(filename):\n",
    "    if not filename.endswith(\".pdf\"):\n",
    "        return\n",
    "\n",
    "    pdf_path = os.path.join(PDF_FOLDER, filename)\n",
    "    try:\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents = loader.load()\n",
    "        chunks = splitter.split_documents(documents)\n",
    "        for i, chunk in enumerate(chunks[:MAX_CHUNKS_PER_PDF]):\n",
    "            try:\n",
    "                llm.invoke(chunk.page_content)\n",
    "                print(f\" {filename} | Chunk {i+1}\")\n",
    "            except Exception as e:\n",
    "                print(f\" Error on chunk {i+1} of {filename}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to process {filename}: {e}\")\n",
    "\n",
    "# Async wrapper that delegates sync task to a thread\n",
    "async def main():\n",
    "    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.endswith(\".pdf\")]\n",
    "    loop = asyncio.get_running_loop()\n",
    "    tasks = [\n",
    "        loop.run_in_executor(executor, process_pdf_sync, f)\n",
    "        for f in pdf_files\n",
    "    ]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()  # Allows nested use of asyncio.run()\n",
    "    asyncio.get_event_loop().run_until_complete(main())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e406aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "CHUNKS_FOLDER = \"chunks\"\n",
    "VECTORSTORE_PATH = \"vectorstore\"\n",
    "\n",
    "#  Recursively load .txt chunks from all subfolders\n",
    "def load_chunks():\n",
    "    print(\" Recursively loading .txt chunks...\")\n",
    "    docs = []\n",
    "    for root, _, files in os.walk(CHUNKS_FOLDER):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                path = os.path.join(root, filename)\n",
    "                try:\n",
    "                    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read().strip()\n",
    "                except UnicodeDecodeError:\n",
    "                    try:\n",
    "                        with open(path, \"r\", encoding=\"latin-1\") as f:\n",
    "                            content = f.read().strip()\n",
    "                        print(f\" Non-UTF8 file read with latin-1: {os.path.relpath(path, CHUNKS_FOLDER)}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\" Skipping unreadable file: {os.path.relpath(path, CHUNKS_FOLDER)} - {e}\")\n",
    "                        continue\n",
    "\n",
    "                if content:\n",
    "                    docs.append(Document(\n",
    "                        page_content=content,\n",
    "                        metadata={\"source\": os.path.relpath(path, CHUNKS_FOLDER)}\n",
    "                    ))\n",
    "                else:\n",
    "                    print(f\" Skipped empty file: {os.path.relpath(path, CHUNKS_FOLDER)}\")\n",
    "    print(f\" Loaded {len(docs)} non-empty documents.\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "#  Embed and prepare vectorstore\n",
    "def prepare_vectorstore(documents):\n",
    "    embedder = OllamaEmbeddings(model=\"llama3.2:1b\")\n",
    "    \n",
    "    if os.path.exists(VECTORSTORE_PATH):\n",
    "        print(\" Loading existing vectorstore...\")\n",
    "        return FAISS.load_local(VECTORSTORE_PATH, embedder)\n",
    "\n",
    "    print(\" Creating new FAISS vectorstore...\")\n",
    "    if not documents:\n",
    "        raise ValueError(\" No documents to index. Check your chunks folder.\")\n",
    "    vs = FAISS.from_documents(documents, embedder)\n",
    "    vs.save_local(VECTORSTORE_PATH)\n",
    "    return vs\n",
    "\n",
    "#  Create the RAG QA chain\n",
    "def create_qa_chain(vectorstore):\n",
    "    llm = OllamaLLM(model=\"llama3.2:1b\")\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    return RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "#  Main loop\n",
    "if __name__ == \"__main__\":\n",
    "    documents = load_chunks()\n",
    "    vectorstore = prepare_vectorstore(documents)\n",
    "    qa_chain = create_qa_chain(vectorstore)\n",
    "\n",
    "    print(\"\\n RAG is ready. Ask anything (type 'exit' to quit):\")\n",
    "    while True:\n",
    "        query = input(\"> \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "        result = qa_chain.run(query)\n",
    "        print(f\" {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8168230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "CHUNKS_FOLDER = \"chunks\"\n",
    "VECTORSTORE_PATH = \"vectorstore\"\n",
    "\n",
    "# Load a single file\n",
    "def load_file(path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read().strip()\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"latin-1\") as f:\n",
    "                content = f.read().strip()\n",
    "            print(f\" Non-UTF8 file read with latin-1: {os.path.relpath(path, CHUNKS_FOLDER)}\")\n",
    "        except Exception as e:\n",
    "            print(f\" Could not read file: {path} - {e}\")\n",
    "            return None\n",
    "    if not content:\n",
    "        print(f\" Skipped empty file: {os.path.relpath(path, CHUNKS_FOLDER)}\")\n",
    "        return None\n",
    "    return Document(page_content=content, metadata={\"source\": os.path.relpath(path, CHUNKS_FOLDER)})\n",
    "\n",
    "#  Load all documents concurrently from chunks/\n",
    "def load_chunks():\n",
    "    print(\" Loading .txt files from 'chunks/' recursively...\")\n",
    "    paths = [\n",
    "        os.path.join(root, file)\n",
    "        for root, _, files in os.walk(CHUNKS_FOLDER)\n",
    "        for file in files if file.endswith(\".txt\")\n",
    "    ]\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        docs = list(executor.map(load_file, paths))\n",
    "    documents = [doc for doc in docs if doc]\n",
    "    print(f\" Loaded {len(documents)} documents.\")\n",
    "    return documents\n",
    "\n",
    "#  Prepare or load FAISS vectorstore\n",
    "def prepare_vectorstore(documents):\n",
    "    embedder = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "    if os.path.exists(VECTORSTORE_PATH):\n",
    "        print(\" Loading existing FAISS vectorstore...\")\n",
    "        return FAISS.load_local(VECTORSTORE_PATH, embedder)\n",
    "\n",
    "    print(\" Building new FAISS vectorstore...\")\n",
    "    if not documents:\n",
    "        raise ValueError(\" No documents found in chunks/.\")\n",
    "    vectorstore = FAISS.from_documents(documents, embedder)\n",
    "    vectorstore.save_local(VECTORSTORE_PATH)\n",
    "    return vectorstore\n",
    "\n",
    "#  Create RAG chain\n",
    "def create_qa_chain(vectorstore):\n",
    "    llm = Ollama(model=\"llama3.2:1b\")\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "    return RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\n",
    "\n",
    "#  Main\n",
    "if __name__ == \"__main__\":\n",
    "    docs = load_chunks()\n",
    "    vectordb = prepare_vectorstore(docs)\n",
    "    qa = create_qa_chain(vectordb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba6ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Load embedding model and FAISS vectorstore\n",
    "embedder = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = FAISS.load_local(\"vectorstore\", embedder, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Create retriever from vectorstore\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Load Ollama LLM\n",
    "llm = Ollama(model=\"llama3.2:1b\")\n",
    "\n",
    "# Load QA chain that only uses provided sources (no external info)\n",
    "qa_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")  # \"stuff\" uses only retrieved context\n",
    "\n",
    "# Define a function that queries only the FAISS data\n",
    "def query_faiss_only(question):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    result = qa_chain({\"input_documents\": docs, \"question\": question})\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "query = \"\"\"\n",
    "Act like a researcher assistant. Summarize what you know about 'dopedsilicon' \n",
    "and return the response in JSON format with the keys: \"definition\", \"applications\", and \"source\".\n",
    "\"\"\"\n",
    "response = query_faiss_only(query)\n",
    "\n",
    "# Output the result and source documents\n",
    "print(response[\"output_text\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
