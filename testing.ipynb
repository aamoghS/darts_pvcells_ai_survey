{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ff7a2b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'pdfs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnest_asyncio\u001b[39;00m                 \n\u001b[0;32m     58\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()  \u001b[38;5;66;03m# Allows nested use of asyncio.run()\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\u001b[38;5;241m.\u001b[39mrun_until_complete(main())\n",
      "File \u001b[1;32mc:\\Users\\bootcamp\\Anaconda3\\Lib\\site-packages\\nest_asyncio.py:90\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[1;32mc:\\Users\\bootcamp\\Anaconda3\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\bootcamp\\Anaconda3\\Lib\\asyncio\\tasks.py:267\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    269\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[1;32mIn[1], line 52\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m---> 52\u001b[0m     pdf_files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(PDF_FOLDER) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     53\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m [process_pdf(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m pdf_files]\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'pdfs'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "PDF_FOLDER = \"pdfs\"\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "MAX_CHUNKS_PER_PDF = 5\n",
    "MAX_WORKERS = 8\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.2:1b\")  # Shared LLM instance\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "executor = ThreadPoolExecutor(max_workers=MAX_WORKERS)\n",
    "\n",
    "def load_and_split_pdf(pdf_path):\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    return chunks[:MAX_CHUNKS_PER_PDF] if MAX_CHUNKS_PER_PDF else chunks\n",
    "\n",
    "def process_chunk_sync(content, filename, i):\n",
    "    try:\n",
    "        llm.invoke(content)\n",
    "        print(f\" {filename} | Chunk {i+1}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error on chunk {i+1} of {filename}: {e}\")\n",
    "\n",
    "async def process_pdf(filename):\n",
    "    if not filename.endswith(\".pdf\"):\n",
    "        return\n",
    "\n",
    "    pdf_path = os.path.join(PDF_FOLDER, filename)\n",
    "    try:\n",
    "        chunks = await asyncio.get_event_loop().run_in_executor(executor, load_and_split_pdf, pdf_path)\n",
    "\n",
    "        tasks = [\n",
    "            asyncio.get_event_loop().run_in_executor(\n",
    "                executor, process_chunk_sync, chunk.page_content, filename, i\n",
    "            )\n",
    "            for i, chunk in enumerate(chunks)\n",
    "        ]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to process {filename}: {e}\")\n",
    "\n",
    "async def main():\n",
    "    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.endswith(\".pdf\")]\n",
    "    tasks = [process_pdf(f) for f in pdf_files]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import nest_asyncio                 \n",
    "    nest_asyncio.apply()  # Allows nested use of asyncio.run()\n",
    "    asyncio.get_event_loop().run_until_complete(main())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ef0826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " JinkoSolar Eagle 54HM G6 Datasheet (420â€“440 W, N-Type TOPCon).pdf | Chunk 4\n",
      " Model_Based_Continuous_Improvement_of_Industrial_p.pdf | Chunk 4\n",
      " JinkoSolar Eagle 72 G6B Datasheet (570â€“590 W, N-Type Bifacial).pdf | Chunk 4\n",
      " 1-s2.0-S1876610215008851-main.pdf | Chunk 5\n",
      " Adani ELAN SHINE TOPCon Datasheet (550â€“575 W, N-Type Bifacial).pdf | Chunk 5\n",
      " 1-s2.0-S1876610215008206-main.pdf | Chunk 5\n",
      " Qcells Q.TRON BLK M-G2+ Series Datasheet (415â€“440 Wp, 2024).pdf | Chunk 1\n",
      " Intl J of Energy Research - 2021 - Gawusu - The dynamics of green supply chain management within the framework of renewable.pdf | Chunk 5\n",
      " Model_Based_Continuous_Improvement_of_Industrial_p.pdf | Chunk 5\n",
      " JinkoSolar Eagle 54HM G6 Datasheet (420â€“440 W, N-Type TOPCon).pdf | Chunk 5\n",
      " JinkoSolar Eagle 72 G6B Datasheet (570â€“590 W, N-Type Bifacial).pdf | Chunk 5\n",
      " Rayzon TOPCon Datasheet (570â€“590 W, N-Type Bifacial, 2024).pdf | Chunk 1\n",
      " Resistive_Power_Loss_Analysis_of_PV_Modules_Made_From_Halved_15.615.6_cm2_Silicon_PERC_Solar_Cells_With_Efficiencies_up_to_20.0.pdf | Chunk 1\n",
      " Silfab SIL-420_430 QD Datasheet (420â€“430 W, N-Type, 2022).pdf | Chunk 1\n",
      " Qcells Q.TRON BLK M-G2+ Series Datasheet (415â€“440 Wp, 2024).pdf | Chunk 2\n",
      " The_Glass-glass_Module_Using_n-type_Bifacial_Solar.pdf | Chunk 1\n",
      " Silfab SIL-520 XM Datasheet (520 W, N-Type Bifacial, 2024).pdf | Chunk 1\n",
      " Resistive_Power_Loss_Analysis_of_PV_Modules_Made_From_Halved_15.615.6_cm2_Silicon_PERC_Solar_Cells_With_Efficiencies_up_to_20.0.pdf | Chunk 2\n",
      " Rayzon TOPCon Datasheet (570â€“590 W, N-Type Bifacial, 2024).pdf | Chunk 2\n",
      " Silfab SIL-420_430 QD Datasheet (420â€“430 W, N-Type, 2022).pdf | Chunk 2\n",
      " Qcells Q.TRON BLK M-G2+ Series Datasheet (415â€“440 Wp, 2024).pdf | Chunk 3\n",
      " The_Glass-glass_Module_Using_n-type_Bifacial_Solar.pdf | Chunk 2\n",
      " Silfab SIL-520 XM Datasheet (520 W, N-Type Bifacial, 2024).pdf | Chunk 2\n",
      " Resistive_Power_Loss_Analysis_of_PV_Modules_Made_From_Halved_15.615.6_cm2_Silicon_PERC_Solar_Cells_With_Efficiencies_up_to_20.0.pdf | Chunk 3\n",
      " Rayzon TOPCon Datasheet (570â€“590 W, N-Type Bifacial, 2024).pdf | Chunk 3\n",
      " Qcells Q.TRON BLK M-G2+ Series Datasheet (415â€“440 Wp, 2024).pdf | Chunk 4\n",
      " Silfab SIL-420_430 QD Datasheet (420â€“430 W, N-Type, 2022).pdf | Chunk 3\n",
      " Silfab SIL-520 XM Datasheet (520 W, N-Type Bifacial, 2024).pdf | Chunk 3\n",
      " The_Glass-glass_Module_Using_n-type_Bifacial_Solar.pdf | Chunk 3\n",
      " Resistive_Power_Loss_Analysis_of_PV_Modules_Made_From_Halved_15.615.6_cm2_Silicon_PERC_Solar_Cells_With_Efficiencies_up_to_20.0.pdf | Chunk 4\n",
      " Qcells Q.TRON BLK M-G2+ Series Datasheet (415â€“440 Wp, 2024).pdf | Chunk 5\n",
      " Silfab SIL-420_430 QD Datasheet (420â€“430 W, N-Type, 2022).pdf | Chunk 4\n",
      " Rayzon TOPCon Datasheet (570â€“590 W, N-Type Bifacial, 2024).pdf | Chunk 4\n",
      " The_Glass-glass_Module_Using_n-type_Bifacial_Solar.pdf | Chunk 4\n",
      " Silfab SIL-520 XM Datasheet (520 W, N-Type Bifacial, 2024).pdf | Chunk 4\n",
      " Silfab SIL-420_430 QD Datasheet (420â€“430 W, N-Type, 2022).pdf | Chunk 5\n",
      " Resistive_Power_Loss_Analysis_of_PV_Modules_Made_From_Halved_15.615.6_cm2_Silicon_PERC_Solar_Cells_With_Efficiencies_up_to_20.0.pdf | Chunk 5\n",
      " The_Glass-glass_Module_Using_n-type_Bifacial_Solar.pdf | Chunk 5\n",
      " Rayzon TOPCon Datasheet (570â€“590 W, N-Type Bifacial, 2024).pdf | Chunk 5\n",
      " Silfab SIL-520 XM Datasheet (520 W, N-Type Bifacial, 2024).pdf | Chunk 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Configuration\n",
    "PDF_FOLDER = \"pdfs\"\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "MAX_CHUNKS_PER_PDF = 5\n",
    "MAX_WORKERS = os.cpu_count() or 8\n",
    "\n",
    "# Shared objects\n",
    "llm = OllamaLLM(model=\"llama3.2:1b\")\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "executor = ThreadPoolExecutor(max_workers=MAX_WORKERS)\n",
    "\n",
    "# Purely synchronous processing for one PDF file\n",
    "def process_pdf_sync(filename):\n",
    "    if not filename.endswith(\".pdf\"):\n",
    "        return\n",
    "\n",
    "    pdf_path = os.path.join(PDF_FOLDER, filename)\n",
    "    try:\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents = loader.load()\n",
    "        chunks = splitter.split_documents(documents)\n",
    "        for i, chunk in enumerate(chunks[:MAX_CHUNKS_PER_PDF]):\n",
    "            try:\n",
    "                llm.invoke(chunk.page_content)\n",
    "                print(f\" {filename} | Chunk {i+1}\")\n",
    "            except Exception as e:\n",
    "                print(f\" Error on chunk {i+1} of {filename}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to process {filename}: {e}\")\n",
    "\n",
    "# Async wrapper that delegates sync task to a thread\n",
    "async def main():\n",
    "    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.endswith(\".pdf\")]\n",
    "    loop = asyncio.get_running_loop()\n",
    "    tasks = [\n",
    "        loop.run_in_executor(executor, process_pdf_sync, f)\n",
    "        for f in pdf_files\n",
    "    ]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()  # Allows nested use of asyncio.run()\n",
    "    asyncio.get_event_loop().run_until_complete(main())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e406aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Recursively loading .txt chunks...\n",
      " Non-UTF8 file read with latin-1: 1-s2.0-S0038092X16303383-main\\chunk_009.txt\n",
      " Non-UTF8 file read with latin-1: 1-s2.0-S0927024815001415-main\\chunk_052.txt\n",
      " Non-UTF8 file read with latin-1: 1-s2.0-S0927024815001415-main\\chunk_059.txt\n",
      " Non-UTF8 file read with latin-1: 1-s2.0-S0927024815001415-main\\chunk_061.txt\n",
      " Non-UTF8 file read with latin-1: 1-s2.0-S0927024815003244-main\\chunk_055.txt\n",
      " Non-UTF8 file read with latin-1: 1-s2.0-S0927024816000313-main\\chunk_065.txt\n",
      " Non-UTF8 file read with latin-1: 1-s2.0-S0927024816300071-main\\chunk_072.txt\n",
      " Non-UTF8 file read with latin-1: 1-s2.0-S1876610215007183-main\\chunk_038.txt\n",
      " Non-UTF8 file read with latin-1: 1-s2.0-S1876610215007420-main\\chunk_030.txt\n",
      " Non-UTF8 file read with latin-1: 1-s2.0-S1876610215007420-main\\chunk_031.txt\n",
      " Non-UTF8 file read with latin-1: 1-s2.0-S1876610215007420-main\\chunk_033.txt\n",
      " Non-UTF8 file read with latin-1: 1-s2.0-S1876610215007420-main\\chunk_036.txt\n",
      " Non-UTF8 file read with latin-1: Intl J of Energy Research - 2021 - Gawusu - The dynamics of green supply chain management within the framework of renewable\\chunk_199.txt\n",
      " Non-UTF8 file read with latin-1: The_Glass-glass_Module_Using_n-type_Bifacial_Solar\\chunk_011.txt\n",
      " Loaded 1045 non-empty documents.\n",
      " Creating new FAISS vectorstore...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "CHUNKS_FOLDER = \"chunks\"\n",
    "VECTORSTORE_PATH = \"vectorstore\"\n",
    "\n",
    "#  Recursively load .txt chunks from all subfolders\n",
    "def load_chunks():\n",
    "    print(\" Recursively loading .txt chunks...\")\n",
    "    docs = []\n",
    "    for root, _, files in os.walk(CHUNKS_FOLDER):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                path = os.path.join(root, filename)\n",
    "                try:\n",
    "                    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read().strip()\n",
    "                except UnicodeDecodeError:\n",
    "                    try:\n",
    "                        with open(path, \"r\", encoding=\"latin-1\") as f:\n",
    "                            content = f.read().strip()\n",
    "                        print(f\" Non-UTF8 file read with latin-1: {os.path.relpath(path, CHUNKS_FOLDER)}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\" Skipping unreadable file: {os.path.relpath(path, CHUNKS_FOLDER)} - {e}\")\n",
    "                        continue\n",
    "\n",
    "                if content:\n",
    "                    docs.append(Document(\n",
    "                        page_content=content,\n",
    "                        metadata={\"source\": os.path.relpath(path, CHUNKS_FOLDER)}\n",
    "                    ))\n",
    "                else:\n",
    "                    print(f\" Skipped empty file: {os.path.relpath(path, CHUNKS_FOLDER)}\")\n",
    "    print(f\" Loaded {len(docs)} non-empty documents.\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "#  Embed and prepare vectorstore\n",
    "def prepare_vectorstore(documents):\n",
    "    embedder = OllamaEmbeddings(model=\"llama3.2:1b\")\n",
    "    \n",
    "    if os.path.exists(VECTORSTORE_PATH):\n",
    "        print(\" Loading existing vectorstore...\")\n",
    "        return FAISS.load_local(VECTORSTORE_PATH, embedder)\n",
    "\n",
    "    print(\" Creating new FAISS vectorstore...\")\n",
    "    if not documents:\n",
    "        raise ValueError(\" No documents to index. Check your chunks folder.\")\n",
    "    vs = FAISS.from_documents(documents, embedder)\n",
    "    vs.save_local(VECTORSTORE_PATH)\n",
    "    return vs\n",
    "\n",
    "#  Create the RAG QA chain\n",
    "def create_qa_chain(vectorstore):\n",
    "    llm = OllamaLLM(model=\"llama3.2:1b\")\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    return RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "#  Main loop\n",
    "if __name__ == \"__main__\":\n",
    "    documents = load_chunks()\n",
    "    vectorstore = prepare_vectorstore(documents)\n",
    "    qa_chain = create_qa_chain(vectorstore)\n",
    "\n",
    "    print(\"\\n RAG is ready. Ask anything (type 'exit' to quit):\")\n",
    "    while True:\n",
    "        query = input(\"> \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "        result = qa_chain.run(query)\n",
    "        print(f\" {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8168230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "CHUNKS_FOLDER = \"chunks\"\n",
    "VECTORSTORE_PATH = \"vectorstore\"\n",
    "\n",
    "# ðŸ§¹ Load a single file\n",
    "def load_file(path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read().strip()\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"latin-1\") as f:\n",
    "                content = f.read().strip()\n",
    "            print(f\" Non-UTF8 file read with latin-1: {os.path.relpath(path, CHUNKS_FOLDER)}\")\n",
    "        except Exception as e:\n",
    "            print(f\" Could not read file: {path} - {e}\")\n",
    "            return None\n",
    "    if not content:\n",
    "        print(f\" Skipped empty file: {os.path.relpath(path, CHUNKS_FOLDER)}\")\n",
    "        return None\n",
    "    return Document(page_content=content, metadata={\"source\": os.path.relpath(path, CHUNKS_FOLDER)})\n",
    "\n",
    "#  Load all documents concurrently from chunks/\n",
    "def load_chunks():\n",
    "    print(\" Loading .txt files from 'chunks/' recursively...\")\n",
    "    paths = [\n",
    "        os.path.join(root, file)\n",
    "        for root, _, files in os.walk(CHUNKS_FOLDER)\n",
    "        for file in files if file.endswith(\".txt\")\n",
    "    ]\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        docs = list(executor.map(load_file, paths))\n",
    "    documents = [doc for doc in docs if doc]\n",
    "    print(f\" Loaded {len(documents)} documents.\")\n",
    "    return documents\n",
    "\n",
    "#  Prepare or load FAISS vectorstore\n",
    "def prepare_vectorstore(documents):\n",
    "    embedder = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "    if os.path.exists(VECTORSTORE_PATH):\n",
    "        print(\" Loading existing FAISS vectorstore...\")\n",
    "        return FAISS.load_local(VECTORSTORE_PATH, embedder)\n",
    "\n",
    "    print(\" Building new FAISS vectorstore...\")\n",
    "    if not documents:\n",
    "        raise ValueError(\" No documents found in chunks/.\")\n",
    "    vectorstore = FAISS.from_documents(documents, embedder)\n",
    "    vectorstore.save_local(VECTORSTORE_PATH)\n",
    "    return vectorstore\n",
    "\n",
    "#  Create RAG chain\n",
    "def create_qa_chain(vectorstore):\n",
    "    llm = Ollama(model=\"llama3.2:1b\")\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "    return RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\n",
    "\n",
    "#  Main\n",
    "if __name__ == \"__main__\":\n",
    "    docs = load_chunks()\n",
    "    vectordb = prepare_vectorstore(docs)\n",
    "    qa = create_qa_chain(vectordb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba6ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Load embedding model and FAISS vectorstore\n",
    "embedder = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = FAISS.load_local(\"vectorstore\", embedder, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Create retriever from vectorstore\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Load Ollama LLM\n",
    "llm = Ollama(model=\"llama3.2:1b\")\n",
    "\n",
    "# Load QA chain that only uses provided sources (no external info)\n",
    "qa_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")  # \"stuff\" uses only retrieved context\n",
    "\n",
    "# Define a function that queries only the FAISS data\n",
    "def query_faiss_only(question):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    result = qa_chain({\"input_documents\": docs, \"question\": question})\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "query = \"\"\"\n",
    "Act like a researcher assistant. Summarize what you know about 'dopedsilicon' \n",
    "and return the response in JSON format with the keys: \"definition\", \"applications\", and \"source\".\n",
    "\"\"\"\n",
    "response = query_faiss_only(query)\n",
    "\n",
    "# Output the result and source documents\n",
    "print(response[\"output_text\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
