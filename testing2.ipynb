{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "598a8bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load_chunks] Loaded 7166 chunks.\n",
      "Answer: The text discusses various aspects of solar cells, including:\n",
      "\n",
      "1. **Passivation techniques**: The importance of passivation in improving the efficiency and durability of solar cells.\n",
      "2. **Contact technologies**: Different types of contacts used in solar cells, such as screen-printed, fired metallization, and tunnel oxide passivated contact (i-TOPCon).\n",
      "3. **Screen-printed solar cells**: Advances in screen printing technology for large-area industrial silicon solar cells with i-TOPCon design.\n",
      "4. **Nanostructured surfaces**: The use of nanostructures to improve the efficiency of solar cells, including thin-film solar cells and nano-wire arrays.\n",
      "\n",
      "Some key findings from the research include:\n",
      "\n",
      "* Screen-printed solar cells can be used to fabricate large-area industrial silicon solar cells with i-TOPCon design.\n",
      "* Nanostructured surfaces, such as nano-wire arrays, can improve the efficiency of solar cells.\n",
      "* The tunnel oxide passivated contact (i-TOPCon) technique can be used to improve the durability and efficiency of solar cells.\n",
      "\n",
      "Overall, the text highlights the importance of advances in technology for increasing the efficiency and reducing costs of solar energy production.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# --- SETTINGS ---\n",
    "CHUNKS_FOLDER = \"chunks\"\n",
    "MAX_WORKERS = 8\n",
    "LLM_MODEL = \"llama3.2:1b\"\n",
    "\n",
    "\n",
    "# --- LOAD TEXT CHUNKS CONCURRENTLY ---\n",
    "def load_chunks() -> List[Document]:\n",
    "    txt_paths = [\n",
    "        os.path.join(root, fn)\n",
    "        for root, _, files in os.walk(CHUNKS_FOLDER)\n",
    "        for fn in files if fn.endswith(\".txt\")\n",
    "    ]\n",
    "\n",
    "    def read_file(path: str):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read().strip()\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                with open(path, \"r\", encoding=\"latin-1\") as f:\n",
    "                    content = f.read().strip()\n",
    "            except:\n",
    "                return None\n",
    "        return Document(page_content=content, metadata={\"source\": path}) if content else None\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        results = list(executor.map(read_file, txt_paths))\n",
    "\n",
    "    docs = [d for d in results if d]\n",
    "    print(f\"[load_chunks] Loaded {len(docs)} chunks.\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "# --- RUN LLM DIRECTLY ON CONCATENATED TEXT ---\n",
    "def run_llm_direct(query: str) -> str:\n",
    "    docs = load_chunks()\n",
    "    if not docs:\n",
    "        return \"[run_llm] No documents found!\"\n",
    "\n",
    "    # Combine all content into one large context string\n",
    "    combined_text = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    prompt = f\"{combined_text}\\n\\nAnswer this: {query}\"\n",
    "\n",
    "    llm = Ollama(model=LLM_MODEL)\n",
    "    response = llm.invoke(prompt)\n",
    "    return response\n",
    "\n",
    "\n",
    "# --- USAGE ---\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"Summarize the key ideas\"\n",
    "    answer = run_llm_direct(query)\n",
    "    print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b117ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bootcamp\\AppData\\Local\\Temp\\ipykernel_33476\\353532311.py:81: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=LLM_MODEL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Raw LLM Output:\n",
      "\n",
      "Here is the extracted and formatted JSON instance according to the provided schema:\n",
      "```\n",
      "{\n",
      "  \"properties\": {\n",
      "    \"title\": \"Title\",\n",
      "    \"last_name\": \"Last Name\",\n",
      "    \"year\": \"Year\",\n",
      "    \"doi\": \"DOI\",\n",
      "    \"research_focus\": \"Research Focus\",\n",
      "    \"key_findings\": \"Key Findings\",\n",
      "    \"device_type\": \"Device Type\",\n",
      "    \"absorber_material\": \"Absorber Material\",\n",
      "    \"absorber_material_term_used\": \"Absorber Material Term Used\",\n",
      "    \"absorber_dopant_material\": \"Absorber Dopant Material\",\n",
      "    \"absorber_dopant_material_term_used\": \"Absorber Dopant Material Term Used\",\n",
      "    \"absorber_dopant_polarity\": \"Absorber Dopant Polarity\",\n",
      "    \"absorber_dopant_polarity_term_used\": \"Absorber Dopant Polarity Term Used\",\n",
      "    \"front_surface_morphology\": \"Front Surface Morphology\",\n",
      "    \"front_surface_morphology_term_used\": \"Front Surface Morphology Term Used\",\n",
      "    \"rear_surface_morphology\": \"Rear Surface Morphology\",\n",
      "    \"rear_surface_morphology_term_used\": \"Rear Surface Morphology Term Used\",\n",
      "    \"front_surface_passivation_material\": \"Front Surface Passivation Material\",\n",
      "    \"front_surface_passivation_material_term_used\": \"Front Surface Passivation Material Term Used\",\n",
      "    \"rear_surface_passivation_material\": \"Rear Surface Passivation Material\",\n",
      "    \"rear_surface_passivation_material_term_used\": \"Rear Surface Passivation Material Term Used\",\n",
      "    \"negative_metallization_material\": \"Negative Metallization Material\",\n",
      "    \"negative_metallization_material_term_used\": \"Negative Metallization Material Term Used\",\n",
      "    \"positive_metallization_material\": \"Positive Metallization Material\",\n",
      "    \"positive_metallization_material_term_used\": \"Positive Metallization Material Term Used\",\n",
      "    \"efficiency_percent\": \"Efficiency Percent\",\n",
      "    \"cell_area_cm2\": \"Cell Area Cm2\",\n",
      "    \"short_circuit_current_a\": \"Short Circuit Current A\",\n",
      "    \"short_circuit_current_density_ma_cm2\": \"Short Circuit Current Density Ma Cm2\",\n",
      "    \"open_circuit_voltage_v\": \"Open Circuit Voltage V\"\n",
      "  },\n",
      "  \"required\": [\"title\"]\n",
      "}\n",
      "```\n",
      "This JSON instance meets the schema requirements, with all properties having a title and type of string. The `required` field is set to an empty array since only one property (`title`) is required.\n",
      "\n",
      "Note that I've used the same titles for all properties as they are mentioned in the original text (e.g., \"Title\", \"Last Name\", etc.). If your JSON schema allows different titles or if you have additional requirements, please let me know and I can adjust the output accordingly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field, field_validator, ValidationError\n",
    "\n",
    "from langchain_ollama import OllamaLLM  # updated import for latest Ollama LLM wrapper\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# --- SETTINGS ---\n",
    "CHUNKS_FOLDER = \"chunkTest\"\n",
    "LLM_MODEL = \"llama3.2:1b\"\n",
    "OUTPUT_CSV = \"pv_extraction_results_ollama.csv\"\n",
    "\n",
    "# --- SCHEMA ---\n",
    "class PVArticleData(BaseModel):\n",
    "    title: str = Field(\"N/A\")\n",
    "    last_name: str = Field(\"N/A\")\n",
    "    year: str = Field(\"N/A\")\n",
    "    doi: str = Field(\"N/A\")\n",
    "    research_focus: str = Field(\"N/A\")\n",
    "    key_findings: str = Field(\"N/A\")\n",
    "    device_type: str = Field(\"N/A\")\n",
    "    absorber_material: str = Field(\"N/A\")\n",
    "    absorber_material_term_used: str = Field(\"N/A\")\n",
    "    absorber_dopant_material: str = Field(\"N/A\")\n",
    "    absorber_dopant_material_term_used: str = Field(\"N/A\")\n",
    "    absorber_dopant_polarity: str = Field(\"N/A\")\n",
    "    absorber_dopant_polarity_term_used: str = Field(\"N/A\")\n",
    "    front_surface_morphology: str = Field(\"N/A\")\n",
    "    front_surface_morphology_term_used: str = Field(\"N/A\")\n",
    "    rear_surface_morphology: str = Field(\"N/A\")\n",
    "    rear_surface_morphology_term_used: str = Field(\"N/A\")\n",
    "    front_surface_passivation_material: str = Field(\"N/A\")\n",
    "    front_surface_passivation_material_term_used: str = Field(\"N/A\")\n",
    "    rear_surface_passivation_material: str = Field(\"N/A\")\n",
    "    rear_surface_passivation_material_term_used: str = Field(\"N/A\")\n",
    "    negative_metallization_material: str = Field(\"N/A\")\n",
    "    negative_metallization_material_term_used: str = Field(\"N/A\")\n",
    "    positive_metallization_material: str = Field(\"N/A\")\n",
    "    positive_metallization_material_term_used: str = Field(\"N/A\")\n",
    "    efficiency_percent: str = Field(\"N/A\")\n",
    "    cell_area_cm2: str = Field(\"N/A\")\n",
    "    short_circuit_current_a: str = Field(\"N/A\")\n",
    "    short_circuit_current_density_ma_cm2: str = Field(\"N/A\")\n",
    "    open_circuit_voltage_v: str = Field(\"N/A\")\n",
    "    fill_factor_percent: str = Field(\"N/A\")\n",
    "\n",
    "    @field_validator(\"*\", mode=\"before\")\n",
    "    def convert_to_string(cls, v):\n",
    "        return str(v) if v is not None else \"N/A\"\n",
    "\n",
    "# --- LOAD CHUNKS ---\n",
    "def load_all_chunks() -> str:\n",
    "    full_text = \"\"\n",
    "    for root, _, files in os.walk(CHUNKS_FOLDER):\n",
    "        for file in sorted(files):\n",
    "            if file.endswith(\".txt\"):\n",
    "                path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read().strip()\n",
    "                except UnicodeDecodeError:\n",
    "                    with open(path, \"r\", encoding=\"latin-1\") as f:\n",
    "                        content = f.read().strip()\n",
    "                if content:\n",
    "                    full_text += f\"\\n\\n--- {file} ---\\n{content}\"\n",
    "    return full_text.strip()\n",
    "\n",
    "# --- PROMPT TEMPLATE ---\n",
    "prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "Extract structured data from the following academic article text on photovoltaic cells.\n",
    "Focus only on the most efficient cell mentioned.\n",
    "\n",
    "Return only a JSON object matching this schema exactly (no explanations or markdown):\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "Schema:\n",
    "{format_instructions}\n",
    "\"\"\")\n",
    "\n",
    "# --- EXTRACT DATA ---\n",
    "def extract_data_from_text(text: str) -> dict | str:\n",
    "    try:\n",
    "        llm = OllamaLLM(model=LLM_MODEL)\n",
    "        parser = PydanticOutputParser(pydantic_object=PVArticleData)\n",
    "\n",
    "        prompt = prompt_template.format(\n",
    "            text=text,\n",
    "            format_instructions=parser.get_format_instructions()\n",
    "        )\n",
    "        \n",
    "        print(\"DEBUG PROMPT (first 1000 chars):\\n\", prompt[:1000])\n",
    "\n",
    "        raw_output = llm.invoke(prompt).strip()\n",
    "        \n",
    "        print(\"\\nðŸ“ Raw LLM Output:\\n\", raw_output[:2000])  # show first 2000 chars max\n",
    "\n",
    "        if not raw_output or raw_output.lower().startswith((\"error\", \"none\", \"no data\")):\n",
    "            return {\"error_type\": \"empty\", \"details\": \"LLM returned empty or irrelevant output.\"}\n",
    "\n",
    "        try:\n",
    "            parsed = json.loads(raw_output)\n",
    "        except json.JSONDecodeError as je:\n",
    "            return {\n",
    "                \"error_type\": \"invalid_json\",\n",
    "                \"details\": f\"JSON parsing failed: {je.msg} (line {je.lineno}, column {je.colno})\",\n",
    "                \"raw_output\": raw_output\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            record = PVArticleData(**parsed)\n",
    "        except ValidationError as ve:\n",
    "            return {\n",
    "                \"error_type\": \"validation_error\",\n",
    "                \"details\": ve.errors(),\n",
    "                \"raw_output\": parsed\n",
    "            }\n",
    "\n",
    "        result = record.model_dump()\n",
    "        result[\"source\"] = \"ALL_CHUNKS\"\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error_type\": \"exception\", \"details\": str(e)}\n",
    "\n",
    "# --- MAIN ---\n",
    "if __name__ == \"__main__\":\n",
    "    full_context = load_all_chunks()\n",
    "    print(\"DEBUG: Loaded text length =\", len(full_context))\n",
    "    print(\"DEBUG: Loaded text snippet:\\n\", full_context[:500])\n",
    "\n",
    "    if not full_context:\n",
    "        print(\"âš ï¸ No content found in any .txt files.\")\n",
    "    else:\n",
    "        result = extract_data_from_text(full_context)\n",
    "\n",
    "        if isinstance(result, dict) and \"error_type\" not in result:\n",
    "            df_new = pd.DataFrame([result])\n",
    "            if os.path.exists(OUTPUT_CSV):\n",
    "                df_existing = pd.read_csv(OUTPUT_CSV)\n",
    "                df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "            else:\n",
    "                df_combined = df_new\n",
    "            df_combined.to_csv(OUTPUT_CSV, index=False)\n",
    "            print(f\"âœ… Saved extracted data to {OUTPUT_CSV}\")\n",
    "        else:\n",
    "            print(f\"\\nâš ï¸ Extraction failed:\")\n",
    "            print(f\"ðŸ” Error Type: {result['error_type']}\")\n",
    "            print(f\"ðŸ“„ Details: {result['details']}\")\n",
    "            if \"raw_output\" in result:\n",
    "                print(f\"ðŸ“ Raw Output:\\n{result['raw_output']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5bedd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d214bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–ˆâ–ˆâ–‘    â–ˆâ–ˆâ–ˆâ–‘ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘     â–ˆâ–ˆâ–‘     â–ˆâ–ˆâ–‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘  \n",
      "â–ˆâ–ˆâ–‘  â–ˆâ–ˆâ–ˆâ–‘     â–ˆâ–ˆâ–‘    â–ˆâ–ˆâ–ˆâ–‘    â–ˆâ–ˆâ–‘   â–ˆâ–ˆâ–ˆâ–ˆâ–‘   â–ˆâ–ˆâ–ˆâ–ˆâ–‘    â–ˆâ–ˆâ–‘         â–ˆâ–ˆâ–‘     â–ˆâ–ˆâ–‘  â–ˆâ–ˆâ–‘     â–ˆâ–ˆâ–‘\n",
      "â–ˆâ–ˆâ–‘â–ˆâ–ˆâ–ˆâ–‘       â–ˆâ–ˆâ–‘   â–ˆâ–ˆâ–ˆâ–‘           â–ˆâ–ˆâ–‘â–ˆâ–ˆâ–‘ â–ˆâ–ˆâ–‘â–ˆâ–ˆâ–‘    â–ˆâ–ˆâ–‘         â–ˆâ–ˆâ–‘      â–ˆâ–ˆâ–‘ â–ˆâ–ˆâ–‘     â–ˆâ–ˆâ–‘\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–‘         â–ˆâ–ˆâ–‘   â–ˆâ–ˆâ–‘           â–ˆâ–ˆâ–ˆâ–‘ â–ˆâ–ˆâ–‘â–ˆâ–ˆâ–‘ â–ˆâ–ˆâ–‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘   â–ˆâ–ˆâ–‘      â–ˆâ–ˆâ–‘ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘  \n",
      "â–ˆâ–ˆâ–‘â–ˆâ–ˆâ–ˆâ–‘       â–ˆâ–ˆâ–‘   â–ˆâ–ˆâ–ˆâ–‘          â–ˆâ–ˆâ–‘  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ â–ˆâ–ˆâ–ˆâ–‘  â–ˆâ–ˆâ–‘         â–ˆâ–ˆâ–‘      â–ˆâ–ˆâ–‘ â–ˆâ–ˆâ–‘     â–ˆâ–ˆâ–‘\n",
      "â–ˆâ–ˆâ–‘  â–ˆâ–ˆâ–ˆâ–‘     â–ˆâ–ˆâ–‘    â–ˆâ–ˆâ–ˆâ–‘    â–ˆâ–ˆâ–‘ â–ˆâ–ˆâ–ˆâ–‘   â–ˆâ–ˆâ–ˆâ–‘   â–ˆâ–ˆâ–‘  â–ˆâ–ˆâ–‘         â–ˆâ–ˆâ–‘     â–ˆâ–ˆâ–‘  â–ˆâ–ˆâ–‘     â–ˆâ–ˆâ–‘\n",
      "â–ˆâ–ˆâ–‘    â–ˆâ–ˆâ–ˆâ–‘ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘  â–ˆâ–ˆâ–‘           â–ˆâ–ˆâ–ˆâ–‘ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_doc_text(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the raw text content from a published or editable Google Doc URL.\n",
    "    For /edit URLs, uses the export?format=txt endpoint.\n",
    "    For /pub URLs, parses the HTML to extract text.\n",
    "    \"\"\"\n",
    "    if \"/edit\" in url:\n",
    "        # Convert to export URL for plain text\n",
    "        url = url.replace(\"/edit\", \"/export?format=txt\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    elif \"/pub\" in url:\n",
    "        # Fetch HTML content and extract paragraphs text\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        return \"\\n\".join(p.get_text() for p in paragraphs)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported URL format. Please provide a published or editable Google Doc link.\")\n",
    "\n",
    "def parse_triplets(text: str):\n",
    "    \"\"\"\n",
    "    Parses the input text to extract (x, y, char) triplets describing the grid.\n",
    "    Assumes each triplet consists of three lines: x, character, y.\n",
    "    \"\"\"\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    triplets = []\n",
    "    i = 0\n",
    "\n",
    "    # Skip lines until the first integer (x coordinate)\n",
    "    while i < len(lines):\n",
    "        try:\n",
    "            int(lines[i])\n",
    "            break\n",
    "        except ValueError:\n",
    "            i += 1\n",
    "\n",
    "    # Parse triplets\n",
    "    while i + 2 < len(lines):\n",
    "        try:\n",
    "            x = int(lines[i])\n",
    "            char = lines[i + 1]\n",
    "            y = int(lines[i + 2])\n",
    "            triplets.append((x, y, char))\n",
    "        except ValueError:\n",
    "            pass\n",
    "        i += 3\n",
    "\n",
    "    return triplets\n",
    "\n",
    "def build_grid(triplets):\n",
    "    \"\"\"\n",
    "    Builds a 2D grid (list of lists) from the triplets, filling unspecified cells with spaces.\n",
    "    \"\"\"\n",
    "    if not triplets:\n",
    "        return []\n",
    "\n",
    "    max_x = max(x for x, _, _ in triplets)\n",
    "    max_y = max(y for _, y, _ in triplets)\n",
    "\n",
    "    # Initialize grid filled with spaces\n",
    "    grid = [[\" \" for _ in range(max_x + 1)] for _ in range(max_y + 1)]\n",
    "\n",
    "    for x, y, char in triplets:\n",
    "        grid[y][x] = char\n",
    "\n",
    "    return grid\n",
    "\n",
    "def print_grid(grid):\n",
    "    \"\"\"\n",
    "    Prints the 2D character grid line by line.\n",
    "    \"\"\"\n",
    "    for row in grid:\n",
    "        print(\"\".join(row))\n",
    "\n",
    "def decode_google_doc(url: str):\n",
    "    \"\"\"\n",
    "    Main function: fetches document text, parses triplets, builds grid, and prints it.\n",
    "    \"\"\"\n",
    "    text = fetch_doc_text(url)\n",
    "    triplets = parse_triplets(text)\n",
    "    grid = build_grid(triplets)\n",
    "    print_grid(grid)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    doc_url = input(\"Enter published Google Doc URL: \").strip()\n",
    "    decode_google_doc(doc_url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
